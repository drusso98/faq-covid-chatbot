{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Health Organization (WHO) COVID-19 Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEX: <a class=\"anchor\" id=\"index\"></a>\n",
    "* [1. Project overview](#first-bullet)\n",
    "* [2. What is a chatbot?](#second-bullet)\n",
    "* [3. Data retrival and intents creation](#third-bullet)\n",
    "    * [Scrapers](#scrapers)\n",
    "    * [Dataset creation (GUI)](#dataset)\n",
    "* [4. Data preprocessing](#fourth-bullet)\n",
    "* [5. Neural Network Architecture](#fifth-bullet)\n",
    "    * [Classes definition](#network_architecture)\n",
    "    * [Classes instantiation](#instantiation)\n",
    "* [6. Training the model](#sixth-bullet)\n",
    "* [7. Chatting](#seventh-bullet)\n",
    "    * [Speech Recognition Module](#speech_recognition)\n",
    "    * [Chatbot GUI](#chatbot_gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to create a Support Chatbot able to answer questions concerning COVID-19 virus. All the answers are taken from the <a href=\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-answers-hub/q-a-detail/coronavirus-disease-covid-19\"> World Health Organization Frequently Asked Questions (FAQ)</a>. This chatbot relies on Natural Language Processing techniques and machine learning algorithms.\n",
    "\n",
    "### Modules:\n",
    "- requests\n",
    "- BeautifoulSoup\n",
    "- json\n",
    "- re\n",
    "- tkinter\n",
    "- numpy\n",
    "- nltk\n",
    "- torch\n",
    "- random\n",
    "- speech_recognition\n",
    "- string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Chatbot? <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "[return to index](#index)\n",
    "\n",
    "A chatbot, or chatterBot, is a software developed to simulate a dialog with a human being. Chatbots are used in dialog systems for various purposes including customer service, request routing, or for information gathering.\n",
    "\n",
    "There are two types of chatbots:\n",
    "- <b>Rule-Based:</b> chatbots based on pre-set rules to which users question must conform in order to get an answer. Rule-Based chatbots can deal only with simple queries.   \n",
    "- <b>Self-Learning:</b> chatbots based on Machine Learning approaches to process users's queries. Particularly, Natural Language Processing techniques (NLP) are adopted for this purpose. There are two types of Self-Learning chatbots:\n",
    "    - <b>Retrieval Based: </b>retrieval-based models that uses some heuristic to select a response from a library of predefined responses.\n",
    "    - <b>Generative: </b>Generative chatbots are able to generate new answers.\n",
    "\n",
    "In this project a Retrival Based chatbot has been implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data retrival and intents creation <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this chatbot is small and restricted to a limited number of topics that users may ask. Those topics are called <b>intents</b> and are structured in this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\"intents\": [\n",
    "        {\"tag\": str,\n",
    "         \"patterns\": [str, str, ...],\n",
    "         \"responses\": [str, str, ...]\n",
    "        },\n",
    "    ...\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>tag:</b> name of the intent \n",
    "- <b>patterns:</b> list of possible questions about the tag \n",
    "- <b>responses:</b> answer to all the \"patterns\" questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Tags</i> used in this project are listed in <a href=\"tags.txt\">covid_faq.txt</a>. <i>Questions</i> are scraped from websites with FAQs about Covid in it. In <a href=\"covid_faq.txt\">covid_faq.txt</a> are listed the urls of the websites scraped for this chatbot. All the <i>answers</i> provided by the chatbot the official one from the <a href=\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-answers-hub/q-a-detail/coronavirus-disease-covid-19\"> World Health Organization</a>.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vxm9jsQ2hCiR"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1608992160542,
     "user": {
      "displayName": "Daniel Russo",
      "photoUrl": "",
      "userId": "01604697782695565650"
     },
     "user_tz": -60
    },
    "id": "YO-ZZd9zhQ2g"
   },
   "outputs": [],
   "source": [
    "def create_intent(question,response,tag):\n",
    "\n",
    "    \"\"\" \n",
    "    Given a pattern, a respose, and a tag, creates an intent (dict). \n",
    "    \"\"\"\n",
    "    d = {\"tag\": tag,\n",
    "         \"patterns\": [question],\n",
    "         \"responses\": [response]\n",
    "         }\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def init_dataset(patterns,responses,tags):\n",
    "\n",
    "    \"\"\" \n",
    "    intialises the dataset with some standard intents and WHO FAQs. \n",
    "    \"\"\"\n",
    "    dataset = { \"intents\" : [\n",
    "        {\"tag\": \"greeting\",\n",
    "         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\"],\n",
    "         \"responses\": [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"],\n",
    "         },\n",
    "        {\"tag\": \"goodbye\",\n",
    "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "         \"responses\": [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\n",
    "         },\n",
    "        {\"tag\": \"thanks\",\n",
    "         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
    "         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
    "         }\n",
    "     ]\n",
    "    }\n",
    "    \n",
    "    #creates an intent for every WHO FAQ\n",
    "    for index in range(len(patterns)):\n",
    "        dataset[\"intents\"].append(create_intent(patterns[index], responses[index], tags[index]))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_source(url_):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a url as input it returns the BeautifulSoup object of it    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url_)\n",
    "        response.raise_for_status()\n",
    "        print(\"%%% Succesfully connected to the url with status code\", response.status_code, \"%%%\\n\")\n",
    "    except:\n",
    "        print(\"%%% Try again. Status code: \", response.status_code, \"%%%\\n\")\n",
    "    return  BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "\n",
    "def raw_data(url,link):\n",
    "    raw = {'name' : url,\n",
    "         'url' : link,\n",
    "         'html' : get_source(link)}\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sCE6RWCb7v3G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%% Succesfully connected to the url with status code 200 %%%\n",
      "\n",
      "%%% Succesfully connected to the url with status code 200 %%%\n",
      "\n",
      "%%% Succesfully connected to the url with status code 200 %%%\n",
      "\n",
      "%%% Succesfully connected to the url with status code 200 %%%\n",
      "\n",
      "%%% Succesfully connected to the url with status code 200 %%%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#open the list of urls to scrape \n",
    "with open('covid_faq.txt', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "websites = [line.strip('\\n').split(',') for line in lines]\n",
    "\n",
    "#raw dataset with name, url and BeautifulSoup object of every website\n",
    "raw_dataset = {'websites' : [raw_data(website[0],website[1]) for website in websites]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1cIrk5IA5Ew"
   },
   "source": [
    "### Scrapers <a class=\"anchor\" id=\"scrapers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1608992166516,
     "user": {
      "displayName": "Daniel Russo",
      "photoUrl": "",
      "userId": "01604697782695565650"
     },
     "user_tz": -60
    },
    "id": "7EQB78BYA4zc"
   },
   "outputs": [],
   "source": [
    "def who_scraper(source):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrapes questions and answers from World Health Organization website FAQ\n",
    "    \"\"\"\n",
    "    \n",
    "    sections = source.find_all(\"div\", class_=\"sf-accordion\")\n",
    "\n",
    "    questions = []\n",
    "    for section in sections:\n",
    "        for question in section.find_all(\"a\", class_=\"sf-accordion__link\"):\n",
    "            questions.append(question.get_text().strip())\n",
    "\n",
    "    answers = []\n",
    "    for section in sections:\n",
    "        for item in section.find_all(\"div\", class_=\"sf-accordion__content\"):\n",
    "            text = re.sub(\"\\n| Â |  +\", \" \", item.get_text(\" \", strip=True))\n",
    "            answers.append(text)\n",
    "\n",
    "    return questions,answers\n",
    "\n",
    "\n",
    "def hopkins_scraper(source):\n",
    "    sections = source.find_all(\"div\", class_=\"rtf\")\n",
    "    questions = []\n",
    "    for section in sections:\n",
    "        if section.find(\"h2\") is not None:\n",
    "            faq = section.find_all(\"h2\")\n",
    "            faq = [question.get_text() for question in faq]\n",
    "            questions.extend(faq)\n",
    "    return questions\n",
    "\n",
    "\n",
    "def nicd_scraper(source):\n",
    "    sections = source.find_all(\"div\", class_=\"elementor-accordion-item\")\n",
    "    questions = []\n",
    "    for section in sections:\n",
    "        for question in section.find_all(\"a\", class_=\"elementor-accordion-title\"):\n",
    "            questions.append(question.get_text().strip().lower())\n",
    "\n",
    "    return questions\n",
    "  \n",
    "\n",
    "def nsw_scraper(source):\n",
    "    elements = source.find(\"div\", class_=\"ms-rtestate-field\").find_all(\"h3\")\n",
    "    questions = [faq.get_text().strip(\"\\n\").replace(\"\\xa0\", \" \") for faq in elements]\n",
    "    return questions\n",
    "\n",
    "\n",
    "def penn_scraper(source):\n",
    "    sections = source.find_all(\"section\", class_=\"js-tabs__content\")\n",
    "    questions = [faq.find(\"h3\").get_text().strip() for faq in sections]\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1608983728367,
     "user": {
      "displayName": "Daniel Russo",
      "photoUrl": "",
      "userId": "01604697782695565650"
     },
     "user_tz": -60
    },
    "id": "z9FFImLuhUPl",
    "outputId": "c67538ad-a055-425b-e3a6-69ee6c84fa52"
   },
   "outputs": [],
   "source": [
    "who_patterns, who_responses = who_scraper(raw_dataset[\"websites\"][0][\"html\"])\n",
    "\n",
    "#opening list of intents' tags\n",
    "with open(\"tags.txt\") as file:\n",
    "    tags = file.read().split(\"\\n\")\n",
    "        \n",
    "dataset = init_dataset(who_patterns, who_responses, tags)\n",
    "\n",
    "with open(\"intents.json\", \"w\") as out:\n",
    "    json.dump(dataset, out)\n",
    "\n",
    "#scraping all the questions     \n",
    "all_questions = []    \n",
    "for item in raw_dataset[\"websites\"]:\n",
    "    if item[\"name\"] == 'hopkins':\n",
    "        all_questions.extend(hopkins_scraper(item[\"html\"]))\n",
    "    elif item[\"name\"] == 'nicd':\n",
    "        all_questions.extend(nicd_scraper(item[\"html\"]))\n",
    "    elif item[\"name\"] == 'nsw':\n",
    "        all_questions.extend(nsw_scraper(item[\"html\"]))\n",
    "    elif item[\"name\"] == 'penn':\n",
    "        all_questions.extend(penn_scraper(item[\"html\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation (GUI) <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(tag, dataset):\n",
    "    \"\"\"\n",
    "    Returns an intent given a tag. \n",
    "    \"\"\"\n",
    "    for item in dataset['intents']:\n",
    "        if item['tag'] == tag:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "window = Tk()\n",
    "window.geometry('800x550')\n",
    "window.title(\"Modify intents\")\n",
    "window.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "#tag selection\n",
    "tags_lbl = Label(window, text=\"Chose a tag to see the related intent\", font=(\"Roboto\", 14))\n",
    "tags_lbl.grid(column=0, row=0)\n",
    "\n",
    "combo_tags = Combobox(window, values=tags, font=(\"Roboto\", 10))\n",
    "\n",
    "def callback(eventObject):\n",
    "    #intent display\n",
    "    selected_tag = combo_tags.get()\n",
    "    intent_text = scrolledtext.ScrolledText(window, width=80, height=10, font=(\"Roboto\", 10))\n",
    "    intent_data = get_intent(selected_tag,dataset)\n",
    "    output = json.dumps(intent_data, indent=True, ensure_ascii=True)\n",
    "    intent_text.insert(INSERT, output)\n",
    "    intent_text.grid(column=0,row=2,pady=10)\n",
    "    \n",
    "    #question list\n",
    "    list_lbl = Label(window, text=\"Choose one or more pattern to add: \", font=(\"Roboto\",10))\n",
    "    list_lbl.grid(row=3,column=0)\n",
    "    \n",
    "    frame = Frame(window)\n",
    "    frame.grid(row=4, column=0, padx=10, pady=10)\n",
    "    \n",
    "    list_patterns = Listbox(frame, width=90, height=10, font=(\"Roboto\", 10), selectmode=MULTIPLE)\n",
    "    list_patterns.pack(side=\"left\", fill=\"y\")\n",
    "\n",
    "    scrollbar = Scrollbar(frame, orient=\"vertical\")\n",
    "    scrollbar.config(command=list_patterns.yview)\n",
    "    scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "    list_patterns.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    for faq in all_questions:\n",
    "        list_patterns.insert(END, faq)\n",
    "    \n",
    "    #user input text area\n",
    "    frame_entry = Frame(window)\n",
    "    frame_entry.grid(row=5, column=0, padx=5, pady=5)\n",
    "    entry_lbl = Label(frame_entry, text=\"Or manually add a new pattern: \", font=[\"Roboto\",10])\n",
    "    entry_lbl.grid(row=0,column=\"0\")\n",
    "    new_faq = Entry(frame_entry,width=50)\n",
    "    new_faq.grid(row=0, column=1)\n",
    "    \n",
    "    #buttons section\n",
    "    frame_btn = Frame(window)\n",
    "    frame_btn.grid(row=6, column=0, padx=5, pady=5)\n",
    "    \n",
    "    def save_intent(dataset,text_area,entry_text,selected_items):\n",
    "        new_patterns = list()\n",
    "        faqs = selected_items.curselection()\n",
    "        new_patterns = [list_patterns.get(faq) for faq in faqs]\n",
    "\n",
    "        if entry_text != \"\":\n",
    "            new_patterns.append(entry_text)\n",
    "\n",
    "        for item in dataset['intents']:\n",
    "            if item['tag'] == selected_tag:\n",
    "                item['patterns'].extend(new_patterns)\n",
    "\n",
    "        intent_data = get_intent(selected_tag,dataset)\n",
    "        output = json.dumps(intent_data, indent=True, ensure_ascii=True)\n",
    "        text_area.delete(\"1.0\", \"end\")\n",
    "        text_area.insert(END, output)\n",
    "        \n",
    "    \n",
    "    btn_update = tk.Button(frame_btn, text=\"Update intent\",\n",
    "                            command= lambda : save_intent(dataset,intent_text,new_faq.get(),list_patterns))\n",
    "    btn_update.grid(row=0, column=1, padx=5)\n",
    "    \n",
    "    def save_dataset(dataset):\n",
    "        with open(\"intents.json\", \"w\") as out:\n",
    "            json.dump(dataset, out)\n",
    "        print(\"dataset saved!\")\n",
    "    \n",
    "    \n",
    "    btn_save = tk.Button(frame_btn, text=\"Save new dataset\", command= lambda : save_dataset(dataset))\n",
    "    btn_save.grid(row=0, column=2, padx=5)\n",
    "\n",
    "combo_tags.grid(row=1, column=0, pady=10)\n",
    "combo_tags.current(1)\n",
    "combo_tags.set(\"Choose a tag\")\n",
    "combo_tags.bind(\"<<ComboboxSelected>>\", callback)\n",
    "\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data prepocessing <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected in the previous section need to be preprocessed in order to be transformed into numerical values and to be used as input of the neural network (defined in section [5](#fifth-bullet)). \n",
    "\n",
    "To do so, first a <b>vocabulary</b> needs to be buildt. Then, using the vocabulary and the intents's pattern as input, numerical values are obtained using the <b>Bag of Words BoW</b> method.\n",
    "\n",
    "<img src=\"vocabulary.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tkozcXbMcKD_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TlrNVxAmcL0h"
   },
   "outputs": [],
   "source": [
    "with open(\"intents/intents.json\", encoding=\"utf8\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# with open(\"intents.json\") as file:\n",
    "#     intents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zXaUryIUiNU4"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "def stemming(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "\n",
    "def bag_of_words(vocabulary, tokenized_sent):\n",
    "    stem_sent = [stemming(word) for word in tokenized_sent]\n",
    "    bag = np.zeros(len(vocabulary), dtype=np.float32)\n",
    "\n",
    "    for index,word in enumerate(vocabulary):\n",
    "        if word in stem_sent:\n",
    "            bag[index] = 1\n",
    "  \n",
    "    return bag\n",
    "\n",
    "\n",
    "def remove_punctuation(vocabulary):\n",
    "    \"\"\"\n",
    "    removes punctuation from the input list and return the stem of the words in it.\n",
    "    \"\"\"\n",
    "    punctuation = set(string.punctuation)\n",
    "    return [stemming(word) for word in vocabulary if word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rYymc7MCfFNr"
   },
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "tags = []\n",
    "pattern_tag = []\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = tokenize(pattern)\n",
    "        vocabulary.extend(tokens)\n",
    "        pattern_tag.append((tokens,tag))\n",
    "\n",
    "#removing punctuation from vocabulary and get the stem of every word\n",
    "vocabulary = remove_punctuation(vocabulary)\n",
    "#deleting duplicates\n",
    "vocabulary = list(set(vocabulary))\n",
    "tags = set(tags)\n",
    "#sorting elements\n",
    "vocabulary = sorted(vocabulary)\n",
    "tags = sorted(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training: \n",
    "    - <i>type</i> : list of np arrays\n",
    "    - <i>content</i> : <b>Bag of Words BoW</b> of every pattern (question)\n",
    "    \n",
    "* output: \n",
    "    - <i>type</i> : list of np arrays \n",
    "    - <i>content</i> : for every pattern (question) the <b>index</b> of the corresponding tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JrdUn3P1-xG9"
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "for pattern,tag in pattern_tag:\n",
    "    bag = bag_of_words(vocabulary, pattern)\n",
    "    training.append(bag)\n",
    "    output.append(tags.index(tag))\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Architecture <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural network: <a class=\"anchor\" id=\"network_architecture\"></a>\n",
    "* <i>input</i> : BoW of the user's query\n",
    "* <i>output</i> : predicted tag \n",
    "* <i>structure</i> :\n",
    "    * input : size equals to the # of words in the vocabulary;\n",
    "    * hidden layers : two layers of size equals to 8\n",
    "    * output : size equals to the # of tags \n",
    "    * activation function: <a href= \"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">rectifier</a>, i.e. the positive part of its argument $$f(x) = x^+ = \\textrm{max}(x,0)$$ $$f(x) = \\begin{cases} x, & \\mbox{if } x \\geq 0 \\\\ 0, & \\mbox{if  } x < 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4711,
     "status": "ok",
     "timestamp": 1608916296211,
     "user": {
      "displayName": "Daniel Russo",
      "photoUrl": "",
      "userId": "01604697782695565650"
     },
     "user_tz": -60
    },
    "id": "tQaCh2GMq6bi",
    "outputId": "ae46b9bb-3d80-44af-e0e6-daa7b3a07db0"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 1500\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(training[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "# print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14737,
     "status": "ok",
     "timestamp": 1608916306253,
     "user": {
      "displayName": "Daniel Russo",
      "photoUrl": "",
      "userId": "01604697782695565650"
     },
     "user_tz": -60
    },
    "id": "GoE5vVGUrE7U",
    "outputId": "d2aa203c-02f6-4c9f-ce3d-0aec701482f3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Chatbot(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Chatbot, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        #activation function: rectifier\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(training)\n",
    "        self.training = training\n",
    "        self.output = output\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.training[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes instantiation <a class=\"anchor\" id=\"instantiation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Chatbot(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot(\n",
      "  (l1): Linear(in_features=110, out_features=8, bias=True)\n",
      "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (l3): Linear(in_features=8, out_features=20, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#summary: model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the model <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1500], Loss: 0.7294\n",
      "Epoch [200/1500], Loss: 0.1913\n",
      "Epoch [300/1500], Loss: 0.0217\n",
      "Epoch [400/1500], Loss: 0.0072\n",
      "Epoch [500/1500], Loss: 0.0033\n",
      "Epoch [600/1500], Loss: 0.1076\n",
      "Epoch [700/1500], Loss: 0.0007\n",
      "Epoch [800/1500], Loss: 0.0009\n",
      "Epoch [900/1500], Loss: 0.0005\n",
      "Epoch [1000/1500], Loss: 0.0003\n",
      "Epoch [1100/1500], Loss: 0.0002\n",
      "Epoch [1200/1500], Loss: 0.1008\n",
      "Epoch [1300/1500], Loss: 0.0002\n",
      "Epoch [1400/1500], Loss: 0.0000\n",
      "Epoch [1500/1500], Loss: 0.0001\n",
      "final loss: 0.0001\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # Updates the parameters\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": vocabulary,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chatting! <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "[return to index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def input_processing(text,vocabulary):\n",
    "    \n",
    "    \"\"\" Input: User's query\n",
    "        Output: Chatbot response OR error message \"\"\"\n",
    "    \n",
    "    text = tokenize(text)\n",
    "    X = bag_of_words(vocabulary, text)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.80:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                return \"BOT:\\n\" + random.choice(intent['responses']) + \"\\n\"\n",
    "    else:\n",
    "        return \"BOT:\\n\" + \"I don't understand. Please, repeat the question.\" + \"\\n\"\n",
    "    \n",
    "    \n",
    "def refresh_chat(query,vocabuary,list_msg):\n",
    "    question = \"YOU:\\n\" + query + \"\\n\" \n",
    "    answer = input_processing(query,vocabulary)\n",
    "    list_msg.insert(END, question)\n",
    "    list_msg.insert(END, answer)\n",
    "    return question,answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition Module <a class=\"anchor\" id=\"speech_recognition\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text(vocabulary,list_msg):\n",
    "    \n",
    "    # create recognizer and mic instances\n",
    "    recognizer = sr.Recognizer()\n",
    "    microphone = sr.Microphone()\n",
    "    \n",
    "    # adjust the recognizer sensitivity to ambient noise and record audio from the microphone\n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "    # set up the response object\n",
    "    response = {\n",
    "        \"success\": True,\n",
    "        \"error\": None,\n",
    "        \"transcription\": None\n",
    "    }    \n",
    "        \n",
    "    # try recognizing the speech in the recording, managing possible errors\n",
    "    try:\n",
    "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
    "    except sr.RequestError:\n",
    "        # API was unreachable or unresponsive\n",
    "        response[\"success\"] = False\n",
    "        response[\"error\"] = \"API unavailable\"\n",
    "    except sr.UnknownValueError:\n",
    "        # speech was unintelligible\n",
    "        response[\"error\"] = \"Unable to recognize speech\"\n",
    "        \n",
    "    if response[\"transcription\"]:\n",
    "        refresh_chat(response[\"transcription\"],vocabulary,list_msg)\n",
    "    elif response[\"success\"] == False:\n",
    "        message = \"\\nERROR: API was unreachable or unresponsive.\\n\"\n",
    "        list_msg.insert(END,message)\n",
    "    elif response[\"error\"]:\n",
    "        message = \"\\nERROR: \" + response[\"error\"] + \".\\n\"\n",
    "        list_msg.insert(END,message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot GUI <a class=\"anchor\" id=\"chatbot_gui\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "# loading the pre-trained model\n",
    "\n",
    "data = torch.load(\"data.pth\")\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = Chatbot(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "# set dropout and batch normalization layers\n",
    "model.eval()\n",
    "\n",
    "window = Tk()\n",
    "window.geometry('700x550')\n",
    "window.title(\"Chatbot\")\n",
    "window.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "\n",
    "tags_lbl = Label(window, text=\"What do you want to know about COVID-19?\", font=(\"Roboto\", 13))\n",
    "tags_lbl.grid(column=0, row=0, pady=15)\n",
    "\n",
    "# chat window\n",
    "frame = Frame(window)\n",
    "frame.grid(row=1, column=0, padx=15, pady=10)\n",
    "list_msg = Text(frame, width=90, height=23, font=(\"Roboto\", 10))\n",
    "list_msg.pack(side=\"left\", fill=\"y\")\n",
    "scrollbar = Scrollbar(frame, orient=\"vertical\")\n",
    "scrollbar.config(command=list_msg.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "list_msg.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "# user query window\n",
    "frame_entry = Frame(window)\n",
    "frame_entry.grid(row=2, column=0, padx=10, pady=5)\n",
    "query = Entry(frame_entry,width=80)\n",
    "query.grid(row=0, column=0, padx=5)\n",
    "btn_audio = tk.Button(frame_entry, text=\"record\", command= lambda: audio_to_text(vocabulary,list_msg))\n",
    "btn_audio.grid(row=0, column=1, padx=2)\n",
    "btn_send = tk.Button(frame_entry, text=\"send\", command= lambda: refresh_chat(query.get(),vocabulary,list_msg))\n",
    "btn_send.grid(row=0, column=2, padx=2)\n",
    "\n",
    "# end conversation and close the window\n",
    "btn_quit = tk.Button(window, text=\"quit\", command=window.destroy)\n",
    "btn_quit.grid(row=3, column=0, pady=15)\n",
    "\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMErk7AKhTX0dtK13C4/Zs8",
   "mount_file_id": "1YQ0_IXQWtZfwvtybM_KQUGeJ3Q_qokUK",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
